{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport cv2\nimport os\nimport tqdm.notebook as tq\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom time import time\nimport matplotlib.pyplot as plt\nimport random\nfrom glob import glob\nimport scipy.io\nimport torch.nn as nn\nimport numpy as np\nfrom ssqueezepy import cwt\nfrom ssqueezepy.visuals import plot, imshow\nimport os\nimport sys\nimport re\nimport pandas as pd\nfrom skimage.transform import resize\nimport warnings\nfrom pathlib import Path \nimport pickle\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:22:31.984034Z","iopub.execute_input":"2023-01-20T15:22:31.984970Z","iopub.status.idle":"2023-01-20T15:22:35.958099Z","shell.execute_reply.started":"2023-01-20T15:22:31.984919Z","shell.execute_reply":"2023-01-20T15:22:35.957153Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!mkdir ~/.kaggle\n!touch ~/.kaggle/kaggle.json\napi_token = {\"username\":\"****\",\"key\":\"****\"}\nimport json\n\nwith open('/root/.kaggle/kaggle.json', 'w') as file:\n    json.dump(api_token, file)\n\n!chmod 600 ~/.kaggle/kaggle.json\n!kaggle competitions download -c alvi-hack-2023","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:22:49.660724Z","iopub.execute_input":"2023-01-20T15:22:49.661092Z","iopub.status.idle":"2023-01-20T15:23:01.876800Z","shell.execute_reply.started":"2023-01-20T15:22:49.661061Z","shell.execute_reply":"2023-01-20T15:23:01.875604Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading alvi-hack-2023.zip to /kaggle/working\n100%|███████████████████████████████████████▉| 229M/229M [00:06<00:00, 38.1MB/s]\n100%|████████████████████████████████████████| 229M/229M [00:06<00:00, 35.0MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!unzip alvi-hack-2023.zip ","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:23:01.879577Z","iopub.execute_input":"2023-01-20T15:23:01.879968Z","iopub.status.idle":"2023-01-20T15:23:07.005365Z","shell.execute_reply.started":"2023-01-20T15:23:01.879924Z","shell.execute_reply":"2023-01-20T15:23:07.004076Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Archive:  alvi-hack-2023.zip\n  inflating: data_submission/00.npz  \n  inflating: data_submission/01.npz  \n  inflating: data_submission/02.npz  \n  inflating: data_submission/03.npz  \n  inflating: data_submission/04.npz  \n  inflating: data_submission/05.npz  \n  inflating: data_submission/06.npz  \n  inflating: data_submission/07.npz  \n  inflating: data_submission/08.npz  \n  inflating: data_submission/09.npz  \n  inflating: data_submission/10.npz  \n  inflating: data_submission/11.npz  \n  inflating: data_submission/12.npz  \n  inflating: data_submission/13.npz  \n  inflating: data_submission/14.npz  \n  inflating: data_train/00.npz       \n  inflating: data_train/01.npz       \n  inflating: data_train/02.npz       \n  inflating: data_train/03.npz       \n  inflating: data_train/04.npz       \n  inflating: data_train/05.npz       \n  inflating: data_train/06.npz       \n  inflating: data_train/07.npz       \n  inflating: data_train/08.npz       \n  inflating: data_train/09.npz       \n  inflating: data_train/10.npz       \n  inflating: data_train/11.npz       \n  inflating: data_train/12.npz       \n  inflating: data_train/13.npz       \n  inflating: data_train/14.npz       \n  inflating: data_train/15.npz       \n  inflating: data_train/16.npz       \n  inflating: data_train/17.npz       \n  inflating: data_train/18.npz       \n  inflating: data_train/19.npz       \n  inflating: data_train/20.npz       \n  inflating: data_train/21.npz       \n  inflating: data_train/22.npz       \n  inflating: data_train/23.npz       \n  inflating: data_train/24.npz       \n  inflating: data_train/25.npz       \n  inflating: data_train/26.npz       \n  inflating: data_train/27.npz       \n  inflating: data_train/28.npz       \n  inflating: data_train/29.npz       \n  inflating: data_train/30.npz       \n  inflating: data_train/31.npz       \n  inflating: data_train/32.npz       \n  inflating: data_train/33.npz       \n  inflating: data_train/34.npz       \n  inflating: data_train/35.npz       \n  inflating: data_train/36.npz       \n  inflating: data_train/37.npz       \n  inflating: data_train/38.npz       \n  inflating: data_train/39.npz       \n  inflating: data_train/40.npz       \n  inflating: data_train/41.npz       \n  inflating: data_train/42.npz       \n  inflating: data_train/43.npz       \n  inflating: data_train/44.npz       \n  inflating: data_train/45.npz       \n  inflating: data_train/46.npz       \n  inflating: data_train/47.npz       \n  inflating: data_train/48.npz       \n  inflating: data_train/49.npz       \n  inflating: data_train/50.npz       \n  inflating: data_train/51.npz       \n  inflating: data_train/52.npz       \n  inflating: data_train/53.npz       \n  inflating: data_train/54.npz       \n  inflating: data_train/55.npz       \n  inflating: data_train/56.npz       \n  inflating: data_train/57.npz       \n","output_type":"stream"}]},{"cell_type":"code","source":"def noisy(image):\n    ch, row,col= image.shape\n    mean = 0\n    var = 0.1\n    sigma = var**0.5\n    gauss = np.random.normal(mean,sigma,(ch,row,col))\n    gauss = gauss.reshape(ch,row,col)\n    noisy = image + gauss\n    return noisy","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:34:08.805906Z","iopub.execute_input":"2023-01-20T15:34:08.806297Z","iopub.status.idle":"2023-01-20T15:34:08.811997Z","shell.execute_reply.started":"2023-01-20T15:34:08.806262Z","shell.execute_reply":"2023-01-20T15:34:08.811037Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class DataReader(Dataset):\n    def __init__(self, path, window, mode='val'):\n        self.window = window\n        self.path = path\n        self.counter = 0\n        self.mode = mode\n        \n        self.x = torch.from_numpy(np.load(path + sorted(os.listdir(path))[0])['data_myo']).type(torch.FloatTensor)\n        self.y = torch.from_numpy(np.load(path + sorted(os.listdir(path))[0])['data_vr']).type(torch.FloatTensor)\n        for i in sorted(os.listdir(path)):\n            if i != sorted(os.listdir(path))[0]:\n                self.x = torch.cat([self.x, torch.from_numpy(np.load(path + i)['data_myo']).type(torch.FloatTensor)], dim=0)\n                self.y = torch.cat([self.y, torch.from_numpy(np.load(path + i)['data_vr']).type(torch.FloatTensor)], dim=0)\n        x_add = self.x[0]\n        for i in range(self.window):\n            self.x = torch.cat([x_add[None], self.x], dim=0)\n        self.x = self.x[:-1]\n        \n        self.xx = self.x[:200][None]\n        n = 1\n        self.yy = self.y[0][None]\n        while n * self.window + self.window < len(self.y):\n            self.xx = torch.cat([self.xx, self.x[n * self.window:n * self.window + self.window ][None]], dim=0)\n            self.yy = torch.cat([self.yy, self.y[n * self.window][None]], dim=0)\n            n += 1\n    def __len__(self):\n        return len(self.yy)\n\n    def __getitem__(self, idx):\n        # print(f'train - {self.counter}:{self.counter + self.window}')\n        # print(f'target - {self.counter}/{len(self.y)}')\n        # y = self.y[idx * self.window, :]\n        # x = self.x[idx * self.window:idx * self.window + self.window, :]\n        x = self.xx[idx]\n        y = self.yy[idx]\n        x = x.permute(1,0)\n        x=convertDF2MNE(x)\n        Wx, scales = cwt(x[0], 'morlet')\n        x=np.abs(Wx)\n        if self.mode == 'train':\n            return noisy(resize(x, (8, 224, 224))), y\n        else:\n            return resize(x, (8, 224, 224)), y","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:35:20.457707Z","iopub.execute_input":"2023-01-20T15:35:20.458091Z","iopub.status.idle":"2023-01-20T15:35:20.471480Z","shell.execute_reply.started":"2023-01-20T15:35:20.458060Z","shell.execute_reply":"2023-01-20T15:35:20.470260Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train_path = '/kaggle/working/data_train/'\ntest_loc = '/kaggle/working/data_test/'","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:23:07.039466Z","iopub.execute_input":"2023-01-20T15:23:07.042050Z","iopub.status.idle":"2023-01-20T15:23:07.051228Z","shell.execute_reply.started":"2023-01-20T15:23:07.042003Z","shell.execute_reply":"2023-01-20T15:23:07.050321Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"os.mkdir('data_test')\nfor i in range(10):\n    os.replace(train_path+f'{i:02}.npz', test_loc+f'{i:02}.npz')","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:23:07.056202Z","iopub.execute_input":"2023-01-20T15:23:07.058746Z","iopub.status.idle":"2023-01-20T15:23:07.065747Z","shell.execute_reply.started":"2023-01-20T15:23:07.058710Z","shell.execute_reply":"2023-01-20T15:23:07.064844Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\nimport mne\ndef convertDF2MNE(sub):\n    info = mne.create_info([str(i) for i in range(len(sub))], ch_types=['eeg'] * len(sub), sfreq=200, verbose=False)\n    data=mne.io.RawArray(sub, info, verbose=False)\n    data.set_eeg_reference(verbose=False)\n    epochs=mne.make_fixed_length_epochs(data,duration=1,overlap=0, verbose=False)\n    return epochs._get_data(verbose=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:23:07.070120Z","iopub.execute_input":"2023-01-20T15:23:07.072637Z","iopub.status.idle":"2023-01-20T15:23:07.725847Z","shell.execute_reply.started":"2023-01-20T15:23:07.072601Z","shell.execute_reply":"2023-01-20T15:23:07.724686Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_ds = DataReader(train_path, 200, mode='train')\ntest_ds = DataReader(test_loc, 200)\n\ntrain_dataloader = DataLoader(train_ds, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_ds, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:47:57.699456Z","iopub.execute_input":"2023-01-20T15:47:57.699756Z","iopub.status.idle":"2023-01-20T15:47:57.777868Z","shell.execute_reply.started":"2023-01-20T15:47:57.699688Z","shell.execute_reply":"2023-01-20T15:47:57.776064Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_22/763921062.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'DataReader' is not defined"],"ename":"NameError","evalue":"name 'DataReader' is not defined","output_type":"error"}]},{"cell_type":"code","source":"model = torch.hub.load(\"pytorch/vision\", \"resnet50\", weights=\"IMAGENET1K_V2\")","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:23:15.697312Z","iopub.execute_input":"2023-01-20T15:23:15.698068Z","iopub.status.idle":"2023-01-20T15:23:17.001798Z","shell.execute_reply.started":"2023-01-20T15:23:15.698025Z","shell.execute_reply":"2023-01-20T15:23:17.000813Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n","output_type":"stream"}]},{"cell_type":"code","source":"model.conv1 = nn.Conv2d(8, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\nmodel.fc = nn.Linear(in_features=2048, out_features=64, bias=True)\n\nmodel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False\n\n\nfor param in model.layer4.parameters():\n    param.requires_grad = True\nfor param in model.avgpool.parameters():\n    param.requires_grad = True\nfor param in model.fc.parameters():\n    param.requires_grad = True\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef train(model, opt, loss_fn, epochs, saver):\n    torch.cuda.empty_cache()\n    tr_loss = []\n    val_loss = []\n    for epoch in tq.tqdm(range(epochs)):\n        tic = time()\n        print('* Epoch %d/%d' % (epoch+1, epochs))\n\n        tr_avg_loss = 0\n        model.train()  # train mode\n        train_ds = DataReader(train_path, 200)\n        data_tr = DataLoader(train_ds, batch_size=64, shuffle=True)\n\n        for X_batch, Y_batch in tq.tqdm(data_tr):\n            X_batch = X_batch.to(device)\n            Y_batch = Y_batch.to(device)\n            opt.zero_grad()\n            Y_pred = model(X_batch)\n            Y_pred = Y_pred.view(-1, 16, 4)\n            loss = loss_fn(Y_pred, Y_batch)\n            loss.backward()\n            opt.step()\n\n            tr_avg_loss += loss.cpu().item() / len(data_tr)\n\n        tr_loss.append(tr_avg_loss)\n        toc = time()\n        print('train loss: %f' % tr_avg_loss)\n\n        avg_loss_val = 0\n        model.eval()\n        test_ds = DataReader(test_loc, 200)\n        data_val = DataLoader(test_ds, batch_size=64, shuffle=False)\n        for X_val, Y_val in tq.tqdm(data_val):\n            with torch.no_grad():\n                Y_hat = model(X_val.to(device)).detach().cpu()\n                Y_hat = Y_hat.view(-1, 16, 4)\n                loss = loss_fn(Y_hat, Y_val)\n                avg_loss_val += loss.item() / len(data_val)\n\n        val_loss.append(avg_loss_val)\n        toc = time()\n        print('val loss: %f' % avg_loss_val)\n        saver.save_best_model(model, avg_loss_val)\n        \n  \n\n      \n    return tr_loss, val_loss\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BestModel():\n    def __init__(self, path, model):\n        self.path = path\n        self.best_loss = 900\n        self.model = model\n    def save_best_model(self, model, loss):\n        if loss < self.best_loss:\n            self.best_loss = loss\n            torch.save(self.model, self.path)\n            print('model saved')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodel = model.to(device)\n\nsaver = BestModel('/kaggle/working/resnet.pt', model)\nmax_epochs = 14\nloss = nn.L1Loss()\noptim = torch.optim.AdamW(model.parameters(), lr=0.00100, weight_decay=0.05)\ntr_loss, val_loss = train(model, optim, loss, max_epochs, saver)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 9))\nplt.plot(tr_loss, label=\"train_loss\")\nplt.plot(val_loss, label=\"val_loss\")\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SubDataReader(Dataset):\n    def __init__(self, path, window):\n        self.window = window\n        self.path = path\n        self.x = torch.from_numpy(np.load(path + sorted(os.listdir(path))[0])['data_myo']).type(torch.FloatTensor)\n        for i in sorted(os.listdir(path)):\n            if i != sorted(os.listdir(path))[0]:\n                self.x = torch.cat([self.x, torch.from_numpy(np.load(path + i)['data_myo']).type(torch.FloatTensor)], dim=0)\n        x_add = self.x[0]\n        for i in range(self.window):\n            self.x = torch.cat([x_add[None], self.x], dim=0)\n    def __len__(self):\n        return len(self.x) - 200\n\n    def __getitem__(self, idx):\n        # print(f'train - {self.counter}:{self.counter + self.window}')\n        # print(f'target - {self.counter}/{len(self.y)}')\n        x = self.x[idx:idx + self.window]\n        x = x.permute(1,0)\n        x=convertDF2MNE(x)\n        Wx, scales = cwt(x[0], 'morlet')\n        x=np.abs(Wx)\n        \n        return resize(x, (8, 224, 224))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_path = '/kaggle/working/data_submission/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = '/kaggle/working/resnet.pt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.load(PATH)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SAVE_PATH = '/kaggle/working/submit.pt'\n\ndef sub(model):\n    torch.cuda.empty_cache()\n\n    model.eval()\n    sub_ds = SubDataReader(sub_path, 200)\n    sub_dataloader = DataLoader(sub_ds, batch_size=64, shuffle=True)\n    first = True\n    for X in tq.tqdm(sub_dataloader):\n        with torch.no_grad():\n            Y_hat = model(X.to(device)).detach().cpu()\n            Y_hat = Y_hat.view(-1, 16, 4)\n            if first:\n                res = Y_hat\n                first = False\n            else:\n                res = torch.cat([res, Y_hat], dim=0)\n            torch.save(res, SAVE_PATH)\n            print(res.shape)\n            \n      \n    return res\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_VAL_FOLDER = '/kaggle/working/data_submission/''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_VAL_FOLDER = Path('/kaggle/working/data_submission/')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SubFileDataReader(Dataset):\n    def __init__(self, file, window):\n        self.window = window\n        self.path = file\n        self.x = torch.from_numpy(np.load(file)['data_myo']).type(torch.FloatTensor)\n        x_add = self.x[0]\n        for i in range(self.window):\n            self.x = torch.cat([x_add[None], self.x], dim=0)\n    def __len__(self):\n        return len(self.x) - 200\n\n    def __getitem__(self, idx):\n        # print(f'train - {self.counter}:{self.counter + self.window}')\n        # print(f'target - {self.counter}/{len(self.y)}')\n        x = self.x[idx:idx + self.window]\n        x = x.permute(1,0)\n        x=convertDF2MNE(x)\n        Wx, scales = cwt(x[0], 'morlet')\n        x=np.abs(Wx)\n        \n        return resize(x, (8, 224, 224))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SAVE_PATH = '/kaggle/working/submit.pt'\n\ndef sub_file(model, p):\n    torch.cuda.empty_cache()\n\n    model.eval()\n    ds = SubFileDataReader(p, 200)\n    sub_dataloader = DataLoader(ds, batch_size=64, shuffle=True)\n    first = True\n    for X in tq.tqdm(sub_dataloader):\n        with torch.no_grad():\n            Y_hat = model(X.to(device)).detach().cpu()\n            Y_hat = Y_hat.view(-1, 16, 4)\n            if first:\n                res = Y_hat\n                first = False\n            else:\n                res = torch.cat([res, Y_hat], dim=0)\n            \n      \n    return res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = '/kaggle/working/resnet.pt'\nmodel = torch.load(PATH)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_preds = []\nmodel = model.to(device)\nprint(\"Start pred\")\nfor p in tq.tqdm(sorted(DATA_VAL_FOLDER.glob('*.npz'))):   \n    with torch.no_grad():\n        res = sub_file(model, p)\n    \n    # Put it to common list\n    list_preds.append(res)\n    print(len(list_preds))\n\n    with open('/kaggle/working/list_preds.pickle', 'wb') as f:\n        pickle.dump(list_preds, f)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_predictions_for_csv(list_predictions):\n    \"\"\"\n    [ [Time, 16, 4 ], ... ]\n    return np array with N values.  \n    \"\"\"\n    result = []\n    for pred in list_predictions: \n        pred = np.reshape(pred[::10], [-1])\n        result.extend(pred)\n    result = np.array(result)\n    return result","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummy_pred = prepare_predictions_for_csv(list_preds)\n\n# There are must be two columns with header Id,Expected\ndf = pd.DataFrame({'Predicted': dummy_pred})\n\n\noutput_folder = Path(f\"/kaggle/working/\")\ndf.to_csv(output_folder / 'submission.csv', index_label = 'Id')","metadata":{},"execution_count":null,"outputs":[]}]}